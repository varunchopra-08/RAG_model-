# -*- coding: utf-8 -*-
"""RAG model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lPq4AYe2ym42yXtU19deF4RAS3dOdM1p
"""

from transformers import AutoTokenizer, DPRQuestionEncoder, T5ForConditionalGeneration, T5Tokenizer
from sentence_transformers import SentenceTransformer, util
import torch

# Load the retriever model
retriever_tokenizer = AutoTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
retriever_model = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base")

# Load the generator model
generator_tokenizer = T5Tokenizer.from_pretrained("t5-small")
generator_model = T5ForConditionalGeneration.from_pretrained("t5-small")

# Load a pre-trained embedding model for semantic retrieval
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Sample knowledge base (documents)
documents = [
    "Python is a high-level programming language known for its readability.",
    "RAG combines document retrieval with generative models for better responses.",
    "Transformers have revolutionized NLP tasks with their attention mechanisms."
]

# Compute document embeddings
doc_embeddings = embedding_model.encode(documents, convert_to_tensor=True)

# Function to retrieve the most relevant document based on cosine similarity
def retrieve_documents(query, docs, doc_embeddings, top_k=1):
    query_embedding = embedding_model.encode(query, convert_to_tensor=True)
    cos_scores = util.cos_sim(query_embedding, doc_embeddings)[0]
    top_results = torch.topk(cos_scores, top_k)

    retrieved_docs = [docs[idx] for idx in top_results.indices]
    return retrieved_docs

# Example query
query = "Tell me about the benefits of RAG models."
retrieved_docs = retrieve_documents(query, documents, doc_embeddings)

# Print the retrieved document
print("Retrieved Document:", retrieved_docs[0])